import streamlit as st
import pandas as pd
from io import StringIO
from datetime import datetime
from youtube_stats_scraper_ui_final_cleaned_strings import (
    extract_video_id, extract_channel_id, get_video_info_batch
)


def process_csv_video_streamlit(df):
    urls = df["url"].dropna().unique()
    video_ids = []
    url_map = {}
    for url in urls:
        vid = extract_video_id(url)
        if vid:
            video_ids.append(vid)
            url_map[vid] = url
    infos = get_video_info_batch(video_ids)

    output = []
    for info in infos:
        output.append({
            "–°—Å—ã–ª–∫–∞": url_map.get(info["ID –≤–∏–¥–µ–æ"], "N/A"),
            "–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞": info["–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞"],
            "–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ": info["–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ"],
            "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã": info["–ü—Ä–æ—Å–º–æ—Ç—Ä—ã"],
            "–î–∞—Ç–∞ —Å–±–æ—Ä–∞": info["–î–∞—Ç–∞ —Å–±–æ—Ä–∞"]
        })

    result_df = pd.DataFrame(output)
    out_file = f"video_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    result_df.to_csv(out_file, index=False, encoding='utf-8-sig')
    return out_file, result_df


def analyze_channels_streamlit(df, keywords):
    urls = df["channel_url"].dropna().unique()
    keywords = [kw.strip().lower() for kw in keywords if kw.strip()]
    all_rows = []

    for idx, url in enumerate(urls):
        st.info(f"–ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–∞ {idx + 1}/{len(urls)}: {url}")
        channel_id = extract_channel_id(url)
        if not channel_id:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å ID –∫–∞–Ω–∞–ª–∞: {url}")
            continue

        uploads_playlist = YOUTUBE.channels().list(part="contentDetails", id=channel_id).execute()
        uploads_id = uploads_playlist["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]

        video_ids = []
        next_token = None
        while len(video_ids) < 20:
            playlist_response = YOUTUBE.playlistItems().list(
                part="contentDetails",
                playlistId=uploads_id,
                maxResults=50,
                pageToken=next_token
            ).execute()
            for item in playlist_response.get("items", []):
                video_ids.append(item["contentDetails"]["videoId"])
                if len(video_ids) >= 20:
                    break
            next_token = playlist_response.get("nextPageToken")
            if not next_token:
                break

        video_stats = get_video_info_batch(video_ids[:20])
        avg_views = sum(int(v["–ü—Ä–æ—Å–º–æ—Ç—Ä—ã"]) for v in video_stats[:10]) // max(1, len(video_stats[:10]))

        for v in video_stats:
            for kw in keywords:
                if kw in v["–û–ø–∏—Å–∞–Ω–∏–µ"].lower():
                    all_rows.append({
                        "–ö–∞–Ω–∞–ª": v["–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞"],
                        "–°—Å—ã–ª–∫–∞": f"https://youtu.be/{v['ID –≤–∏–¥–µ–æ']}",
                        "–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ": v["–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ"],
                        "–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ": kw,
                        "–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã": avg_views
                    })

    result_df = pd.DataFrame(all_rows)
    out_file = f"channel_keyword_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    result_df.to_csv(out_file, index=False, encoding='utf-8-sig')
    return out_file, result_df


# Streamlit UI
st.set_page_config(page_title="YouTube Stats Scraper", layout="wide")
st.title("üìä YouTube Stats Scraper")

option = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:", ["–ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ", "–ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–æ–≤"])

if option == "–ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ":
    st.header("üîç –ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ")
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Å –∫–æ–ª–æ–Ω–∫–æ–π 'url'", type="csv")

    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        if "url" not in df.columns:
            st.error("‚ùå CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É 'url'")
        else:
            if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑"):
                out_file, result_df = process_csv_video_streamlit(df)
                st.success("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω")
                st.download_button("üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç", data=result_df.to_csv(index=False), file_name=out_file, mime="text/csv")
                st.dataframe(result_df)

elif option == "–ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–æ–≤":
    st.header("üß™ –ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–æ–≤")
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Å –∫–æ–ª–æ–Ω–∫–æ–π 'channel_url'", type="csv")
    keywords_input = st.text_area("–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ):")
    keywords = keywords_input.strip().splitlines()

    if uploaded_file and keywords:
        df = pd.read_csv(uploaded_file)
        if "channel_url" not in df.columns:
            st.error("‚ùå CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É 'channel_url'")
        else:
            if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑"):
                out_file, result_df = analyze_channels_streamlit(df, keywords)
                st.success("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω")
                st.download_button("üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç", data=result_df.to_csv(index=False), file_name=out_file, mime="text/csv")
                st.dataframe(result_df)
